{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T14:55:28.557145Z",
     "iopub.status.busy": "2025-03-14T14:55:28.556832Z",
     "iopub.status.idle": "2025-03-14T14:55:28.564072Z",
     "shell.execute_reply": "2025-03-14T14:55:28.562879Z",
     "shell.execute_reply.started": "2025-03-14T14:55:28.557093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "torch==2.3.0\n",
    "torchtext==0.18\n",
    "pandas\n",
    "sentencepiece\n",
    "tqdm\n",
    "wandb\n",
    "sacrebleu==2.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:42:29.448011Z",
     "iopub.status.busy": "2025-03-13T07:42:29.447599Z",
     "iopub.status.idle": "2025-03-13T07:45:23.926740Z",
     "shell.execute_reply": "2025-03-13T07:45:23.925462Z",
     "shell.execute_reply.started": "2025-03-13T07:42:29.447972Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==2.3.0 (from -r requirements.txt (line 1))\n",
      "  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting torchtext==0.18 (from -r requirements.txt (line 2))\n",
      "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.2.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.67.1)\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.19.1)\n",
      "Collecting sacrebleu==2.3.1 (from -r requirements.txt (line 7))\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->-r requirements.txt (line 1)) (2024.12.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==2.3.0 (from torch==2.3.0->-r requirements.txt (line 1))\n",
      "  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18->-r requirements.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.18->-r requirements.txt (line 2)) (1.26.4)\n",
      "Collecting portalocker (from sacrebleu==2.3.1->-r requirements.txt (line 7))\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->-r requirements.txt (line 7)) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->-r requirements.txt (line 7)) (0.9.0)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->-r requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->-r requirements.txt (line 7)) (5.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->-r requirements.txt (line 1)) (12.6.85)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2025.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (5.9.5)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (2.11.0a2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (6.0.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (1.3.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->-r requirements.txt (line 6)) (75.1.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6)) (4.0.11)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchtext==0.18->-r requirements.txt (line 2)) (2.4.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 6)) (2.29.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18->-r requirements.txt (line 2)) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.18->-r requirements.txt (line 2)) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->-r requirements.txt (line 1)) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 6)) (5.0.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchtext==0.18->-r requirements.txt (line 2)) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchtext==0.18->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchtext==0.18->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchtext==0.18->-r requirements.txt (line 2)) (2024.2.0)\n",
      "Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m916.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: triton, portalocker, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext, sacrebleu\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
      "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
      "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n",
      "    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.5.1+cu121\n",
      "    Uninstalling torch-2.5.1+cu121:\n",
      "      Successfully uninstalled torch-2.5.1+cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\n",
      "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 portalocker-3.1.1 sacrebleu-2.3.1 torch-2.3.0 torchtext-0.18.0 triton-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:45:23.928271Z",
     "iopub.status.busy": "2025-03-13T07:45:23.927981Z",
     "iopub.status.idle": "2025-03-13T07:45:24.898807Z",
     "shell.execute_reply": "2025-03-13T07:45:24.897984Z",
     "shell.execute_reply.started": "2025-03-13T07:45:23.928244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dir = \"/kaggle/input/de2en-translation/data\"\n",
    "dst_dir = \"data\"\n",
    "\n",
    "os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(src_dir):\n",
    "    src_file = os.path.join(src_dir, file_name)\n",
    "    dst_file = os.path.join(dst_dir, file_name)\n",
    "    \n",
    "    shutil.copy(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:45:24.899951Z",
     "iopub.status.busy": "2025-03-13T07:45:24.899713Z",
     "iopub.status.idle": "2025-03-13T07:45:24.904457Z",
     "shell.execute_reply": "2025-03-13T07:45:24.903449Z",
     "shell.execute_reply.started": "2025-03-13T07:45:24.899929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"src\", exist_ok=True)\n",
    "os.makedirs(\"data/train_inference\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:45:24.905755Z",
     "iopub.status.busy": "2025-03-13T07:45:24.905401Z",
     "iopub.status.idle": "2025-03-13T07:45:33.906003Z",
     "shell.execute_reply": "2025-03-13T07:45:33.905155Z",
     "shell.execute_reply.started": "2025-03-13T07:45:24.905722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdzhr\u001b[0m (\u001b[33mkdzhr-hse-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "wandb_api_key = user_secrets.get_secret(\"wandb\")\n",
    "wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from https://github.com/pytorch/examples/tree/main/language_translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using code from https://github.com/pytorch/examples/tree/main/language_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile src/data.py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "\n",
    "class DataIter:\n",
    "    def __init__(self, src_path, tgt_path):\n",
    "        with open(src_path, \"r\") as src_file:\n",
    "            self.src_list = [line for line in src_file.readlines()]\n",
    "\n",
    "        with open(tgt_path, \"r\") as tgt_file:\n",
    "            self.tgt_list = [line for line in tgt_file.readlines()]\n",
    "\n",
    "        assert len(self.src_list) == len(self.tgt_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return (self.src_list[index], self.tgt_list[index])\n",
    "\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __call__(self, line):\n",
    "        return line.split()\n",
    "\n",
    "\n",
    "def _yield_tokens(iterable_data, tokenizer, index):\n",
    "    for data in iterable_data:\n",
    "        yield tokenizer(data[index])\n",
    "\n",
    "\n",
    "def get_all_lines(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line[:-1] for line in f.readlines()]\n",
    "\n",
    "\n",
    "def get_data(config):\n",
    "    special_symbols = {\n",
    "        \"<unk>\":0,\n",
    "        \"<pad>\":1,\n",
    "        \"<bos>\":2,\n",
    "        \"<eos>\":3\n",
    "    }\n",
    "\n",
    "    src_train_lines = get_all_lines(config.locs.src_train_path)\n",
    "    tgt_train_lines = get_all_lines(config.locs.tgt_train_path)\n",
    "    src_val_lines = get_all_lines(config.locs.src_val_path)\n",
    "    tgt_val_lines = get_all_lines(config.locs.tgt_val_path)\n",
    "\n",
    "    src_lines = src_train_lines + src_val_lines\n",
    "    tgt_lines = tgt_train_lines + tgt_val_lines\n",
    "    \n",
    "    np.random.seed(52)\n",
    "    indices = np.arange(len(src_lines))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    lines = [(src_lines[ind], tgt_lines[ind]) for ind in indices]\n",
    "\n",
    "    print(lines[:5])\n",
    "    \n",
    "    val_size = int(len(src_lines) * config.train.val_size)\n",
    "    valid_iterator, train_iterator = lines[:val_size], lines[val_size:]\n",
    "    \n",
    "    test_iterator = [(line, line) for line in get_all_lines(config.locs.src_test_path)]\n",
    "\n",
    "    src_tokenizer, tgt_tokenizer = SimpleTokenizer(), SimpleTokenizer()\n",
    "\n",
    "    src_vocab = build_vocab_from_iterator(\n",
    "        _yield_tokens(train_iterator, src_tokenizer, 0),\n",
    "        min_freq=config.train.src_min_freq,\n",
    "        specials=list(special_symbols.keys()),\n",
    "        special_first=True\n",
    "    )\n",
    "\n",
    "    tgt_vocab = build_vocab_from_iterator(\n",
    "        _yield_tokens(train_iterator, tgt_tokenizer, 1),\n",
    "        min_freq=config.train.tgt_min_freq,\n",
    "        specials=list(special_symbols.keys()),\n",
    "        special_first=True\n",
    "    )\n",
    "\n",
    "    src_vocab.set_default_index(special_symbols[\"<unk>\"])\n",
    "    tgt_vocab.set_default_index(special_symbols[\"<unk>\"])\n",
    "\n",
    "    def _seq_transform(*transforms):\n",
    "        def func(txt_input):\n",
    "            for transform in transforms:\n",
    "                txt_input = transform(txt_input)\n",
    "            return txt_input\n",
    "        return func\n",
    "\n",
    "    def _tensor_transform(token_ids):\n",
    "        return torch.cat(\n",
    "            (torch.tensor([special_symbols[\"<bos>\"]]),\n",
    "             torch.tensor(token_ids),\n",
    "             torch.tensor([special_symbols[\"<eos>\"]]))\n",
    "        )\n",
    "\n",
    "    src_lang_transform = _seq_transform(src_tokenizer, src_vocab, _tensor_transform)\n",
    "    tgt_lang_transform = _seq_transform(tgt_tokenizer, tgt_vocab, _tensor_transform)\n",
    "    \n",
    "    def _collate_fn(batch):\n",
    "        src_batch, tgt_batch = [], []\n",
    "        for src_sample, tgt_sample in batch:\n",
    "            src_batch.append(src_lang_transform(src_sample.rstrip(\"\\n\")))\n",
    "            tgt_batch.append(tgt_lang_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "        src_batch = pad_sequence(src_batch, padding_value=special_symbols[\"<pad>\"], batch_first=True)\n",
    "        tgt_batch = pad_sequence(tgt_batch, padding_value=special_symbols[\"<pad>\"], batch_first=True)\n",
    "        return src_batch, tgt_batch\n",
    "\n",
    "    train_dataloader = DataLoader(train_iterator, batch_size=config.train.batch_size, collate_fn=_collate_fn)\n",
    "    valid_dataloader = DataLoader(valid_iterator, batch_size=config.train.batch_size, collate_fn=_collate_fn)\n",
    "    test_dataloader = DataLoader(test_iterator, batch_size=config.train.batch_size, collate_fn=_collate_fn)\n",
    "\n",
    "    return train_dataloader, valid_dataloader, test_dataloader, src_vocab, tgt_vocab, src_lang_transform, tgt_lang_transform, special_symbols, train_iterator, valid_iterator, test_iterator\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(size, device):\n",
    "    mask = (torch.triu(torch.ones((size, size), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt, pad_idx, device):\n",
    "    src_seq_len = src.shape[1]\n",
    "    tgt_seq_len = tgt.shape[1]\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=device).type(torch.bool)\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len, device)\n",
    "    \n",
    "    src_padding_mask = (src == pad_idx)\n",
    "    tgt_padding_mask = (tgt == pad_idx)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile src/model.py\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torch import nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_size,\n",
    "        dropout,\n",
    "        maxlen=5000\n",
    "    ):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(0)\n",
    "                     \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:, :token_embedding.size(1), :])\n",
    "\n",
    "\n",
    "def noisy_emb(emb, noise):\n",
    "    return emb + torch.randn_like(emb) * noise\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            embed_size,\n",
    "            num_heads,\n",
    "            src_vocab_size,\n",
    "            tgt_vocab_size,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            emb_noise,\n",
    "        ):\n",
    "        super(Translator, self).__init__()\n",
    "\n",
    "        self.emb_noise = emb_noise\n",
    "\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, embed_size)\n",
    "\n",
    "        self.pos_enc = PositionalEncoding(embed_size, dropout)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Linear(embed_size, tgt_vocab_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "\n",
    "        src_emb = self.pos_enc(self.src_embedding(src))\n",
    "        tgt_emb = self.pos_enc(self.tgt_embedding(trg))\n",
    "\n",
    "        if self.training and self.emb_noise != 0:\n",
    "            src_emb = noisy_emb(src_emb, self.emb_noise)\n",
    "            tgt_emb = noisy_emb(tgt_emb, self.emb_noise)\n",
    "        \n",
    "        outs = self.transformer(\n",
    "            src_emb,\n",
    "            tgt_emb,\n",
    "            src_mask[0] if len(src_mask.shape) == 3 else src_mask,\n",
    "            tgt_mask[0] if len(tgt_mask.shape) == 3 else tgt_mask,\n",
    "            None,\n",
    "            src_padding_mask,\n",
    "            tgt_padding_mask,\n",
    "            memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        return self.ff(outs)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        embed = self.src_embedding(src)\n",
    "        pos_enc = self.pos_enc(embed)\n",
    "        return self.transformer.encoder(pos_enc, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        embed = self.tgt_embedding(tgt)\n",
    "        pos_enc = self.pos_enc(embed)\n",
    "        return self.transformer.decoder(pos_enc, memory, tgt_mask)\n",
    "\n",
    "\n",
    "class ParallelTranslator(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            embed_size,\n",
    "            num_heads,\n",
    "            src_vocab_size,\n",
    "            tgt_vocab_size,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            emb_noise,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = Translator(\n",
    "            num_encoder_layers,\n",
    "            num_decoder_layers,\n",
    "            embed_size,\n",
    "            num_heads,\n",
    "            src_vocab_size,\n",
    "            tgt_vocab_size,\n",
    "            dim_feedforward,\n",
    "            dropout,\n",
    "            emb_noise,\n",
    "        )\n",
    "        self.par_model = nn.DataParallel(self.model)\n",
    "    \n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        return self.par_model(src, trg, src_mask.unsqueeze(0).repeat(src.shape[0], 1, 1), tgt_mask.unsqueeze(0).repeat(src.shape[0], 1, 1), src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, src, src_mask):\n",
    "        self.model.eval()\n",
    "        return self.model.encode(src, src_mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        self.model.eval()\n",
    "        return self.model.decode(tgt, memory, tgt_mask)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ff(self, x):\n",
    "        self.model.eval()\n",
    "        return self.model.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "# Using code from https://github.com/pytorch/examples/tree/main/language_translation\n",
    "\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import random\n",
    "import json\n",
    "from time import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import wandb\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.model import Translator, ParallelTranslator\n",
    "from src.data import get_data, create_mask, generate_square_subsequent_mask\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "\n",
    "BOS_STRING = \"<bos>\"\n",
    "EOS_STRING = \"<eos>\"\n",
    "PAD_STRING = \"<pad>\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# We'll use this common hack to aesthetically access config elements\n",
    "class DotDict(dict):\n",
    "    def __getattr__(self, key):\n",
    "        assert key in self\n",
    "        value = self.get(key)\n",
    "        if isinstance(value, dict):\n",
    "            return DotDict(value)\n",
    "        return value\n",
    "\n",
    "\n",
    "class CosineScheduler:\n",
    "    def __init__(self, optimizer, init_lr=1e-6, warmup_epochs=20, decay_epochs=180, lr=4e-3, decay_lr=1e-6):\n",
    "        self.optimizer = optimizer\n",
    "        self.init_lr = init_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.lr = lr\n",
    "        self.decay_lr = decay_lr\n",
    "        self.last_lr = None\n",
    "\n",
    "    def get_last_lr(self):\n",
    "        return [self.last_lr]\n",
    "    \n",
    "    def update_optimizer(self, lr):\n",
    "        self.last_lr = lr\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr\n",
    "    \n",
    "    def step(self, epoch):\n",
    "        if epoch < self.warmup_epochs:\n",
    "            self.update_optimizer(self.init_lr + epoch / self.warmup_epochs * (self.lr - self.init_lr))\n",
    "        else:\n",
    "            epoch -= self.warmup_epochs\n",
    "            self.update_optimizer(self.lr * (1 + np.cos(np.pi * epoch / self.decay_epochs)) / 2)\n",
    "\n",
    "\n",
    "def greedy_decode_multi(model, src_batch, src_mask, special_symbols, max_length):\n",
    "    model.eval()\n",
    "    batch_size = src_batch.shape[0]\n",
    "    memory = model.encode(src_batch, src_mask)\n",
    "\n",
    "    tgt_batch = torch.ones((batch_size, 1), dtype=torch.long).fill_(special_symbols[BOS_STRING]).to(DEVICE)\n",
    "\n",
    "    finished_lines = torch.zeros(batch_size, dtype=torch.bool).to(DEVICE)\n",
    "\n",
    "    for _ in range(max_length-1):\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_batch.size(1), DEVICE)\n",
    "        output = model.decode(tgt_batch, memory, tgt_mask)\n",
    "        prob = model.ff(output[:, -1])\n",
    "        \n",
    "        next_tokens = prob.argmax(dim=1)\n",
    "        next_tokens[finished_lines] = special_symbols[PAD_STRING]\n",
    "        finished_lines |= (next_tokens == special_symbols[EOS_STRING])\n",
    "        tgt_batch = torch.cat([tgt_batch, next_tokens.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        if finished_lines.all():\n",
    "            break\n",
    "            \n",
    "    return tgt_batch\n",
    "\n",
    "\n",
    "# it's really slow, but smh gets the best score\n",
    "def beam_decode_naive(model, src, src_mask, special_symbols, max_length, beam_topk, all_topk):\n",
    "    model.eval()\n",
    "    memory = model.encode(src, src_mask)\n",
    "    \n",
    "    start_token = torch.ones(1, 1).fill_(special_symbols[BOS_STRING]).type(torch.long).to(DEVICE)\n",
    "    nodes = [(start_token, 0)]    \n",
    "    for _ in range(max_length - 2):\n",
    "        nodes_new = list()\n",
    "        all_ended = True\n",
    "        for seq, score in nodes:\n",
    "            if seq[0, -1] == special_symbols[EOS_STRING]:\n",
    "                nodes_new.append((seq, score))\n",
    "            else:\n",
    "                all_ended = False\n",
    "                tgt_mask = generate_square_subsequent_mask(seq.size(1), DEVICE)\n",
    "                output = model.decode(seq, memory, tgt_mask)\n",
    "                logits = model.ff(output[:, -1])\n",
    "                \n",
    "                probs = F.log_softmax(logits, dim=1)[0]\n",
    "                topk_scores, topk_tokens = torch.topk(probs, beam_topk)\n",
    "\n",
    "                for cur_score, token_num in zip(topk_scores, topk_tokens):\n",
    "                    token = torch.ones(1, 1).fill_(token_num).type(torch.long).to(DEVICE)\n",
    "                    nodes_new.append((torch.cat((seq, token), dim=1), score + cur_score))\n",
    "        if all_ended:\n",
    "            break\n",
    "        nodes_new.sort(key=lambda x: x[1], reverse=True)\n",
    "        nodes = nodes_new[:all_topk]\n",
    "    return nodes[0][0]\n",
    "\n",
    "\n",
    "def beam_decode(model, src, src_mask, special_symbols, max_length, beam_topk, all_topk):\n",
    "    model.eval()\n",
    "    memory = model.encode(src.repeat(all_topk, 1), src_mask)\n",
    "    \n",
    "    tgt_batch = torch.ones(all_topk, 1).fill_(special_symbols[BOS_STRING]).type(torch.long).to(DEVICE)\n",
    "    scores = torch.zeros(1, 1).type(torch.float64).to(DEVICE)\n",
    "\n",
    "    best_end_score = None\n",
    "    end_seq = None\n",
    "\n",
    "    end_scores = list()\n",
    "    \n",
    "    for len_i in range(max_length - 2):\n",
    "        cur_cnt = scores.shape[0]\n",
    "        nodes_new = list()\n",
    "        all_ended = True\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt_batch.size(1), DEVICE)\n",
    "        output = model.decode(tgt_batch, memory, tgt_mask)\n",
    "        logits = model.ff(output[:, -1])\n",
    "        \n",
    "        topk_scores, topk_tokens = torch.topk(F.log_softmax(logits[:cur_cnt], dim=1), beam_topk, dim=1)\n",
    "        topk_scores += scores\n",
    "\n",
    "        next_size = min(all_topk, beam_topk * cur_cnt)\n",
    "        new_scores, raw_top_ind = torch.topk(topk_scores.flatten(), next_size)\n",
    "        top_ind_i, top_ind_j = raw_top_ind // beam_topk, raw_top_ind % beam_topk\n",
    "\n",
    "        new_scores = new_scores.unsqueeze(1)\n",
    "\n",
    "        cur_tokens = topk_tokens[top_ind_i, top_ind_j].unsqueeze(1)\n",
    "        end_mask = cur_tokens == special_symbols[EOS_STRING]\n",
    "        \n",
    "        new_tokens = torch.ones(all_topk, 1).fill_(special_symbols[BOS_STRING]).type(torch.long).to(DEVICE)\n",
    "        new_tokens[:next_size] = topk_tokens[top_ind_i, top_ind_j].unsqueeze(1)\n",
    "        \n",
    "        new_batch = torch.ones(all_topk, len_i + 1).fill_(special_symbols[BOS_STRING]).type(torch.long).to(DEVICE)\n",
    "        new_batch[:next_size] = tgt_batch[top_ind_i]\n",
    "        \n",
    "        tgt_batch = torch.cat((new_batch, new_tokens), dim=1)\n",
    "        scores = new_scores\n",
    "\n",
    "        end_mask = new_tokens[:next_size] == special_symbols[EOS_STRING]\n",
    "        if end_mask.any():\n",
    "            indices = np.arange(scores.shape[0])[end_mask.flatten().cpu()]\n",
    "            ind_max = indices[scores[end_mask].flatten().argmax().item()]\n",
    "            assert end_mask[ind_max]\n",
    "            end_scores += list(scores[end_mask].flatten())\n",
    "            end_scores = sorted(end_scores, reverse=True)[:all_topk]\n",
    "            if best_end_score is None or scores[ind_max] > best_end_score:\n",
    "                best_end_score = scores[ind_max]\n",
    "                end_seq = tgt_batch[ind_max].unsqueeze(0)\n",
    "            scores[end_mask] -= 1e30\n",
    "\n",
    "        comb_scores = sorted(list(scores.flatten()) + end_scores, reverse=True)\n",
    "        if len(comb_scores) > all_topk:\n",
    "            threshold = comb_scores[all_topk]\n",
    "            scores[scores <= threshold] -= 1e30\n",
    "        \n",
    "        if best_end_score is not None and best_end_score > scores.max():\n",
    "            break\n",
    "            \n",
    "    ind_scores = scores.argmax()\n",
    "    res = end_seq if best_end_score is not None and best_end_score > scores[ind_scores] else tgt_batch[ind_scores].unsqueeze(0)\n",
    "    if res[0, -1] != special_symbols[EOS_STRING]:\n",
    "        end_token = torch.ones(1, 1).fill_(special_symbols[EOS_STRING]).type(torch.long).to(DEVICE)\n",
    "        res = torch.cat((res, end_token), dim=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def beam_decode_multi(model, src_batch, src_mask, special_symbols, max_lengths, beam_topk, all_topk):\n",
    "    max_length = max_lengths.max().item()\n",
    "    tgt_batch = torch.ones(src_batch.shape[0], max_length).fill_(special_symbols[PAD_STRING]).type(torch.long).to(DEVICE)\n",
    "    for i in range(src_batch.shape[0]):\n",
    "        res = beam_decode(model, src_batch[i].unsqueeze(0), src_mask, special_symbols, max_lengths[i], beam_topk, all_topk)\n",
    "        tgt_batch[i:i+1, :res.shape[1]] = res\n",
    "    return tgt_batch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_translations(model, dl, special_symbols, tgt_vocab, gen_method, gen_config):\n",
    "    translations = list()\n",
    "    original_sentences = list()\n",
    "    for src_batch, _ in tqdm(dl):\n",
    "        src_mask = torch.zeros((src_batch.shape[1], src_batch.shape[1]), dtype=torch.bool)\n",
    "        \n",
    "        src_batch = src_batch.to(DEVICE)\n",
    "        src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "        max_length = src_batch.shape[1] + 5\n",
    "\n",
    "        max_lengths = torch.zeros(src_batch.shape[0], dtype=torch.int32)\n",
    "        for i in range(src_batch.shape[0]):\n",
    "            cur_max_length = (src_batch[i] == special_symbols[EOS_STRING]).nonzero()\n",
    "            if cur_max_length.numel() == 0:\n",
    "                cur_max_length = max_length\n",
    "            else:\n",
    "                cur_max_length = cur_max_length[0].item() + 5\n",
    "            max_lengths[i] = cur_max_length\n",
    "        \n",
    "        if gen_method == \"greedy\":\n",
    "            tgt_batch = greedy_decode_multi(model, src_batch, src_mask, special_symbols, max_length)\n",
    "        elif gen_method == \"beam\":\n",
    "            tgt_batch = beam_decode_multi(\n",
    "                model,\n",
    "                src_batch,\n",
    "                src_mask,\n",
    "                special_symbols,\n",
    "                max_lengths,\n",
    "                beam_topk=gen_config.beam.beam_topk,\n",
    "                all_topk=gen_config.beam.all_topk,\n",
    "            )\n",
    "\n",
    "        for i, tgt_tokens in enumerate(tgt_batch):\n",
    "            cur_max_length = max_lengths[i]\n",
    "            output_as_list = list(tgt_tokens.cpu().numpy())[:cur_max_length]\n",
    "            output_list_words = filter(lambda elem: elem not in special_symbols, tgt_vocab.lookup_tokens(output_as_list))\n",
    "            translation = \" \".join(output_list_words)\n",
    "            print(translation)\n",
    "            translations.append(translation)\n",
    "\n",
    "    return translations\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_metrics(model, val_dl, tgt_vocab, special_symbols, valid_iterator, gen_method, gen_config):\n",
    "    model.eval()\n",
    "    translations = generate_translations(model, val_dl, special_symbols, tgt_vocab, gen_method, gen_config)\n",
    "    original_sentences = [tgt for src, tgt in valid_iterator]\n",
    "    for ind in random.choices(range(len(original_sentences)), k=5):\n",
    "        print(f\"original_sentences[{ind}]: {original_sentences[ind]}\")\n",
    "        print(f\"translations[{ind}]: {translations[ind]}\")\n",
    "\n",
    "    bleu_scores = [\n",
    "        sacrebleu.sentence_bleu(translation, [reference]).score\n",
    "        for translation, reference in zip(translations, original_sentences)\n",
    "    ]\n",
    "    \n",
    "    metric_values = {\n",
    "        # \"BLEU\": sacrebleu.corpus_bleu(translations, [[elem] for elem in original_sentences]),\n",
    "        \"BLEU\": sum(bleu_scores) / len(bleu_scores),\n",
    "    }\n",
    "    return metric_values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_translations(model, dl, tgt_vocab, special_symbols, file_path, gen_method, gen_config):\n",
    "    model.eval()\n",
    "    translations = generate_translations(model, dl, special_symbols, tgt_vocab, gen_method, gen_config)\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"\\n\".join(translations) + \"\\n\")\n",
    "\n",
    "\n",
    "def inference(config, model_path):\n",
    "    _, val_dl, test_dl, src_vocab, tgt_vocab, src_transform, _, special_symbols, _, valid_iterator, _ = get_data(config)\n",
    "\n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    model = (ParallelTranslator if config.train.parallel else Translator)(\n",
    "        num_encoder_layers=config.model.enc_layers,\n",
    "        num_decoder_layers=config.model.dec_layers,\n",
    "        embed_size=config.model.embed_size,\n",
    "        num_heads=config.model.attn_heads,\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        dim_feedforward=config.model.dim_feedforward,\n",
    "        dropout=config.model.dropout,\n",
    "        emb_noise=config.model.emb_noise,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    # print(\"Metrics on val set:\")\n",
    "    # print(calc_metrics(model, val_dl, tgt_vocab, special_symbols, valid_iterator, config.gen.inference, config.gen))\n",
    "    \n",
    "    output_lines = generate_translations(model, test_dl, special_symbols, tgt_vocab, config.gen.inference, config.gen)\n",
    "        \n",
    "    with open(config.locs.tgt_test_path, \"w\") as file:\n",
    "        file.write(\"\\n\".join(output_lines) + \"\\n\")\n",
    "    \n",
    "\n",
    "def train(model, train_dl, loss_fn, optim, scheduler, epoch, special_symbols, config):    \n",
    "    # Object for accumulating losses\n",
    "    losses = 0\n",
    "    iters = len(train_dl)\n",
    "\n",
    "    model.train()\n",
    "    for i, (src, tgt) in tqdm(enumerate(train_dl), ascii=True, total=len(train_dl)):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        \n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, special_symbols[\"<pad>\"], DEVICE)\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        optim.zero_grad()\n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(epoch + i / iters)\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dl))\n",
    "\n",
    "\n",
    "def validate(model, valid_dl, loss_fn, special_symbols):    \n",
    "    losses = 0\n",
    "    model.eval()\n",
    "    for src, tgt in tqdm(valid_dl):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input, special_symbols[\"<pad>\"], DEVICE)\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[:, 1:]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(valid_dl))\n",
    "\n",
    "\n",
    "def main(config, load_weights):\n",
    "    os.makedirs(config.locs.logging_dir, exist_ok=True)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(filename=config.locs.logging_dir + \"log.txt\", level=logging.INFO)\n",
    "\n",
    "    console = logging.StreamHandler()\n",
    "    console.setLevel(logging.INFO)\n",
    "    logging.getLogger().addHandler(console)\n",
    "\n",
    "    logging.info(f\"Translation task: {config.lang.src} -> {config.lang.tgt}\")\n",
    "    logging.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "    train_dl, valid_dl, test_dl, src_vocab, tgt_vocab, _, _, special_symbols, _, valid_iterator, _ = get_data(config)\n",
    "\n",
    "    logging.info(\"Loaded data\")\n",
    "\n",
    "    src_vocab_size = len(src_vocab)\n",
    "    tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "    logging.info(f\"{config.lang.src} vocab size: {src_vocab_size}\")\n",
    "    logging.info(f\"{config.lang.tgt} vocab size: {tgt_vocab_size}\")\n",
    "\n",
    "    model = (ParallelTranslator if config.train.parallel else Translator)(\n",
    "        num_encoder_layers=config.model.enc_layers,\n",
    "        num_decoder_layers=config.model.dec_layers,\n",
    "        embed_size=config.model.embed_size,\n",
    "        num_heads=config.model.attn_heads,\n",
    "        src_vocab_size=src_vocab_size,\n",
    "        tgt_vocab_size=tgt_vocab_size,\n",
    "        dim_feedforward=config.model.dim_feedforward,\n",
    "        dropout=config.model.dropout,\n",
    "        emb_noise=config.model.emb_noise,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    if load_weights:\n",
    "        model.load_state_dict(torch.load(config.locs.model_path))\n",
    "    \n",
    "    logging.info(\"Model created... starting training!\")\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(\n",
    "        ignore_index=special_symbols[\"<pad>\"],\n",
    "        label_smoothing=config.train.label_smoothing,\n",
    "    )\n",
    "\n",
    "    if config.train.optim.name == \"Adam\":\n",
    "        opt_params = config.train.optim.adam\n",
    "        optim = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=opt_params.lr,\n",
    "            betas=(opt_params.beta1,opt_params.beta2),\n",
    "            eps=opt_params.eps,\n",
    "            weight_decay=opt_params.weight_decay,\n",
    "        )\n",
    "    \n",
    "    if config.train.scheduler.name == \"Cosine\":\n",
    "        sch_params = config.train.scheduler.cosine\n",
    "        scheduler = CosineScheduler(\n",
    "            optim,\n",
    "            init_lr=sch_params.init_lr,\n",
    "            warmup_epochs=sch_params.warmup_epochs,\n",
    "            decay_epochs=sch_params.decay_epochs,\n",
    "            lr=sch_params.lr,\n",
    "            decay_lr=sch_params.decay_lr,\n",
    "        )\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    best_val_loss = 1e6\n",
    "    \n",
    "    for idx, epoch in enumerate(range(1, config.train.epochs+1)):\n",
    "\n",
    "        start_time = time()\n",
    "        train_loss = train(model, train_dl, loss_fn, optim, scheduler, epoch, special_symbols, config)\n",
    "        epoch_time = time() - start_time\n",
    "        val_loss   = validate(model, valid_dl, loss_fn, special_symbols)\n",
    "        metrics    = calc_metrics(model, valid_dl, tgt_vocab, special_symbols, valid_iterator, config.gen.train, config.gen)\n",
    "        \n",
    "        bleu_score = metrics[\"BLEU\"]\n",
    "\n",
    "        mod_str = f\"e{epoch}_val{val_loss:.3f}_BLEU{bleu_score:.3f}\"\n",
    "        torch.save(model.state_dict(), config.locs.logging_dir + mod_str + \".pt\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            logging.info(\"New best model, saving...\")\n",
    "            # torch.save(model.state_dict(), config.locs.logging_dir + \"best.pt\")\n",
    "\n",
    "        save_path = config.locs.tgt_test_dir + mod_str + \".en\"\n",
    "        save_translations(model, test_dl, tgt_vocab, special_symbols, save_path, config.gen.train, config.gen)\n",
    "        wandb.save(save_path)\n",
    "\n",
    "        # torch.save(model.state_dict(), config.locs.logging_dir + \"last.pt\")\n",
    "\n",
    "        logger.info(f\"Epoch: {epoch}\\n\\tTrain loss: {train_loss:.3f}\\n\\tVal loss: {val_loss:.3f}\\n\\tEpoch time = {epoch_time:.1f} seconds\\n\\tETA = {epoch_time*(config.train.epochs-idx-1):.1f} seconds\\n\\tMetrics:{metrics}\")\n",
    "\n",
    "        log_dict = {\n",
    "            \"Epoch\": epoch,\n",
    "            \"Train loss\": train_loss,\n",
    "            \"Val loss\": val_loss,\n",
    "            \"Epoch time\": epoch_time,\n",
    "            **metrics,\n",
    "        }\n",
    "\n",
    "        if scheduler is not None:\n",
    "            log_dict[\"Learning rate\"] = scheduler.get_last_lr()[0]\n",
    "        \n",
    "        wandb.log(log_dict)\n",
    "\n",
    "    torch.save(model.state_dict(), config.locs.logging_dir + \"last.pt\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    wandb.finish()\n",
    "\n",
    "    random.seed(52)\n",
    "    torch.manual_seed(52)\n",
    "    \n",
    "    parser = ArgumentParser(\n",
    "        prog=\"Machine Translator training and inference\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\"--config\", type=str, default=\"config.json\")\n",
    "    parser.add_argument(\"--exp_name\", type=str, default=\"???\")\n",
    "    parser.add_argument(\"--inference\", action=\"store_true\")\n",
    "    parser.add_argument(\"--model_path\", type=str, default=\"NONE\")\n",
    "    parser.add_argument(\"--load\", action=\"store_true\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    with open(args.config) as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    if args.inference:\n",
    "        inference(DotDict(config), args.model_path)\n",
    "    else:    \n",
    "        wandb.init(project='DL_bhw2', config=config, name=args.exp_name)\n",
    "        config = DotDict(wandb.config)\n",
    "        DEVICE = torch.device(\"cuda\" if config.train.backend == \"gpu\" and torch.cuda.is_available() else \"cpu\")    \n",
    "        main(config, args.load)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:45:33.948001Z",
     "iopub.status.busy": "2025-03-13T07:45:33.947612Z",
     "iopub.status.idle": "2025-03-13T07:45:33.963309Z",
     "shell.execute_reply": "2025-03-13T07:45:33.962448Z",
     "shell.execute_reply.started": "2025-03-13T07:45:33.947965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.json\n",
    "{\n",
    "    \"lang\": {\n",
    "        \"src\": \"de\",\n",
    "        \"tgt\": \"en\"\n",
    "    },\n",
    "    \"locs\": {\n",
    "        \"src_train_path\": \"data/train.de-en.de\",\n",
    "        \"src_val_path\": \"data/val.de-en.de\",\n",
    "        \"tgt_train_path\": \"data/train.de-en.en\",\n",
    "        \"tgt_val_path\": \"data/val.de-en.en\",\n",
    "        \"logging_dir\": \"saves/\",\n",
    "        \n",
    "        \"src_test_path\": \"data/test1.de-en.de\",\n",
    "        \"tgt_test_path\": \"data/test1.de-en.en\",\n",
    "        \"tgt_test_dir\": \"data/train_inference/\"\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"epochs\": 50,\n",
    "        \"val_size\": 0.1,\n",
    "        \"optim\": {\n",
    "            \"name\": \"Adam\",\n",
    "            \"adam\": {\n",
    "                \"lr\": 3e-4,\n",
    "                \"beta1\": 0.9,\n",
    "                \"beta2\": 0.98,\n",
    "                \"eps\": 1e-9,\n",
    "                \"weight_decay\": 0\n",
    "            },\n",
    "        },\n",
    "        \"scheduler\": {\n",
    "            \"name\": \"Cosine\",\n",
    "            \"cosine\": {\n",
    "                \"init_lr\": 1e-6,\n",
    "                \"warmup_epochs\": 5,\n",
    "                \"decay_epochs\": 45,\n",
    "                \"lr\": 2e-3,\n",
    "                \"decay_lr\": 1e-5\n",
    "            }\n",
    "        },\n",
    "        \"batch_size\": 128,\n",
    "        \"src_min_freq\": 7,\n",
    "        \"tgt_min_freq\": 7,\n",
    "        \"backend\": \"gpu\",\n",
    "        \"parallel\": true,\n",
    "        \"label_smoothing\": 0.1\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"attn_heads\": 4,\n",
    "        \"enc_layers\": 4,\n",
    "        \"dec_layers\": 4,\n",
    "        \"embed_size\": 128,\n",
    "        \"dim_feedforward\": 256,\n",
    "        \"dropout\": 0.1,\n",
    "        \"emb_noise\": 0.2\n",
    "    },\n",
    "    \"gen\": {\n",
    "    \"train\": \"greedy\",\n",
    "    \"inference\": \"beam\",\n",
    "    \"greedy\": {\n",
    "        \"meow\": \"meow\"\n",
    "    },\n",
    "    \"beam\": {\n",
    "        \"beam_topk\": 3,\n",
    "        \"all_topk\": 8\n",
    "    }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T07:47:36.647999Z",
     "iopub.status.busy": "2025-03-13T07:47:36.647619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkdzhr\u001b[0m (\u001b[33mkdzhr-hse-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250313_074739-rgys72xq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33med4_mf7_en02\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/kdzhr-hse-university/DL_bhw2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/kdzhr-hse-university/DL_bhw2/runs/rgys72xq\u001b[0m\n",
      "Translation task: de -> en\n",
      "Using device: cuda\n",
      "[('also wenn diese vorhersagen korrekt sind , wird diese lücke sich nicht schließen .', 'so if these predictions are accurate , that gap is not going to close .'), ('wir nehmen das gerüst und bepflanzen es mit zellen ; und hier sehen sie , wie sich die klappensegel öffnen und schließen .', 'we take the scaffold , we seed it with cells , and you can now see here , the valve leaflets opening and closing .'), ('für mich ist das also ein tolles produkt .', \"so it 's , for me , a great product .\"), ('der reiz einer anderen welt ist oft einer der gründe , warum mit dem paranormalen experimentiert wird .', 'the allure of another world is something that people say is part of why they want to dabble in the paranormal .'), ('\" ja , dem stimme ich zu . also , wo ist das problem ? \"', 'and i said , \" yes , i agree with that . so what \\'s the problem ? \"')]\n",
      "Loaded data\n",
      "de vocab size: 18310\n",
      "en vocab size: 14643\n",
      "Model created... starting training!\n",
      "  0%|                                                  | 0/1385 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:5137: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.81it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[19263]: well , that 's where you have to go backstage and ask the magician .\n",
      "translations[19263]: and we 're going to be a lot of the , and we can be going to be\n",
      "original_sentences[1073]: and i 'm slamming it together so hard that that hydrogen fuses together , and in the process it has some byproducts , and i utilize those byproducts .\n",
      "translations[1073]: i 'm going to be going to be a , and i think , and i think , and i think , you can be going to be going\n",
      "original_sentences[14288]: of those who do have access to just that unpaid leave , most women can 't afford to take much of it at all .\n",
      "translations[14288]: and the of the , the is a , and the , and the is a , and the\n",
      "original_sentences[9527]: and they 're rushing around all stressed out .\n",
      "translations[9527]: you 're going to be a , and you can be going\n",
      "original_sentences[18542]: but if you got attacked , that was something you could do something about .\n",
      "translations[18542]: and if you can be going to be a little bit of the , and you can be going to be\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 1\n",
      "\tTrain loss: 6.033\n",
      "\tVal loss: 5.396\n",
      "\tEpoch time = 238.9 seconds\n",
      "\tETA = 11707.6 seconds\n",
      "\tMetrics:{'BLEU': 3.29516032318353}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.18it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:12<00:00, 11.89it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[8010]: right so , finally , an english game that india usurped a little bit , but t20 is going to be the next missionary in the world .\n",
      "translations[8010]: so , in a , a , but a , but a , but the of the world , the , the , the\n",
      "original_sentences[18099]: i actually came across this fascinating study by the pew center this week that revealed that an active facebook user is three times as likely as a non-internet user to believe that most people are trustworthy .\n",
      "translations[18099]: this was the first time i 've been in the , the of the , which is a , the , the , the , the people who is not going to be able to be able to\n",
      "original_sentences[3133]: abed had been less handsome than substantial , with thick black hair and a full face and a wide neck .\n",
      "translations[3133]: and then , some of these things , when a , and a , and a , and a little bit of a\n",
      "original_sentences[19587]: i think the good news for me would be if we could go back and talk about the unleashing , the continuation of the unleashing of human potential .\n",
      "translations[19587]: and in my case , i 'm going to be a little bit of this , and we were going to be able to be able to be able to\n",
      "original_sentences[8131]: so we 're gonna play something together anyway .\n",
      "translations[8131]: but we 're going to be able to be able to be able\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.60it/s]\n",
      "Epoch: 2\n",
      "\tTrain loss: 5.208\n",
      "\tVal loss: 4.743\n",
      "\tEpoch time = 238.0 seconds\n",
      "\tETA = 11421.7 seconds\n",
      "\tMetrics:{'BLEU': 6.143198092147281}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.58it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[3606]: so what 's going on ?\n",
      "translations[3606]: because what happens ? because of course , of course\n",
      "original_sentences[6814]: but i wonder who remembers joe darby , the very obedient , good soldier who found those photographs and handed them in .\n",
      "translations[6814]: but i say , who was very , the very , the very , the , the , and they were , and they were\n",
      "original_sentences[12420]: there 's a piece by a composer , an american composer called john cage .\n",
      "translations[12420]: there 's a piece of a , a , a company called , ,\n",
      "original_sentences[673]: but the great thing about imagining learning as cartography , instead of imagining it as arbitrary hurdles that you have to jump over , is that you see a bit of coastline , and that makes you want to see more .\n",
      "translations[673]: it 's great , like , as a , or a single one of the most of the most that you have to see , because you want to see a , you want to\n",
      "original_sentences[7623]: how are they maintaining it in a particular straight line ?\n",
      "translations[7623]: how do you take a sense of a certain , just a , a\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 3\n",
      "\tTrain loss: 4.682\n",
      "\tVal loss: 4.244\n",
      "\tEpoch time = 238.7 seconds\n",
      "\tETA = 11219.8 seconds\n",
      "\tMetrics:{'BLEU': 10.304119260196842}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.18it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.83it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:43<00:00,  1.49it/s]\n",
      "original_sentences[218]: and it 's 10 triliion dollars .\n",
      "translations[218]: and it 's 10 trillion dollars . and the\n",
      "original_sentences[1365]: so they get immune to sounds that scare them after awhile .\n",
      "translations[1365]: so they 're going to be able to be able to be able to be by\n",
      "original_sentences[3353]: it goes up by 15 percent , and you have a 15 percent savings on the infrastructure .\n",
      "translations[3353]: percent of 15 percent . and they have 15 percent of the infrastructure , and they have\n",
      "original_sentences[4541]: anyway , a goat 's eyes are like a child 's eyes .\n",
      "translations[4541]: in every case , the eyes are a of the eyes of a child , and the\n",
      "original_sentences[5402]: and again , some of these may be far-fetched , but i figured if there 's anywhere to present far-fetched ideas , it 's here at ted , so thank you very much .\n",
      "translations[5402]: and also , if some of my ideas are , where if i didn 't think about ted , i could imagine you , many of the ,\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.62it/s]\n",
      "Epoch: 4\n",
      "\tTrain loss: 4.285\n",
      "\tVal loss: 3.905\n",
      "\tEpoch time = 238.1 seconds\n",
      "\tETA = 10951.0 seconds\n",
      "\tMetrics:{'BLEU': 14.704067888352164}\n",
      "100%|#######################################| 1385/1385 [03:41<00:00,  6.25it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:12<00:00, 12.34it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:43<00:00,  1.49it/s]\n",
      "original_sentences[15321]: take marriage for example .\n",
      "translations[15321]: let 's take a moment as a result , and we\n",
      "original_sentences[8191]: that means that in a population where malaria has gone down all the way , and there 's few people remaining with parasites , that the dogs can find these people , we can treat them with anti-malarial drugs , and give the final blow to malaria .\n",
      "translations[8191]: that means that in a population , in the malaria , in the malaria , and only some people have no people have no human beings , we can find these people , we can be able to be able to be able to\n",
      "original_sentences[16882]: i 'm often asked , \" is digital media replacing the museum ? \"\n",
      "translations[16882]: i 'm going to ask , \" are the digital media in the museum of the museum ? \"\n",
      "original_sentences[17691]: and the poor man said , \" and you mean god didn 't answer my pleas ? \"\n",
      "translations[17691]: and the poor man said , \" and you say , \" and you say , god didn 't have my ? \"\n",
      "original_sentences[17626]: but then something wonderful happened : i was out shopping , as i tend to do , and i came across a bootleg copy of the official gay agenda .\n",
      "translations[17626]: but then something wonderful , i was a little bit of how often was , and then a version of the fbi 's , \" , \"\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.62it/s]\n",
      "Epoch: 5\n",
      "\tTrain loss: 4.016\n",
      "\tVal loss: 3.704\n",
      "\tEpoch time = 234.9 seconds\n",
      "\tETA = 10571.8 seconds\n",
      "\tMetrics:{'BLEU': 18.055973713726413}\n",
      "100%|#######################################| 1385/1385 [03:42<00:00,  6.22it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:12<00:00, 11.93it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:43<00:00,  1.49it/s]\n",
      "original_sentences[18659]: consider for example transporting goods .\n",
      "translations[18659]: so let 's take a , and for example ,\n",
      "original_sentences[11338]: and i knew i would be singing about a half-octave higher than normal , because i was nervous .\n",
      "translations[11338]: and i realized that i 'm going to sing a half more than normal , because i was nervous , and i was , and i\n",
      "original_sentences[13215]: and toward the end of writing that book there was a documentary that came out .\n",
      "translations[13215]: and when i had the book almost , a came out , and i got a\n",
      "original_sentences[17356]: the eventual goal is that we 'll get this into patients .\n",
      "translations[17356]: the goal is that it 's , in patients , and it 's a\n",
      "original_sentences[10705]: that means i had to go 600 miles away from my hometown to the river ganges to take a holy dip .\n",
      "translations[10705]: that meant , i had to go to to the of my hometown of the , and a famous\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.62it/s]\n",
      "Epoch: 6\n",
      "\tTrain loss: 3.838\n",
      "\tVal loss: 3.574\n",
      "\tEpoch time = 235.9 seconds\n",
      "\tETA = 10381.1 seconds\n",
      "\tMetrics:{'BLEU': 17.95983979492889}\n",
      " 30%|###########8                            | 410/1385 [01:05<02:40,  6.07it/s]"
     ]
    }
   ],
   "source": [
    "!python3 main.py --exp_name ed4_mf7_en02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python3 main.py --inference --model_path saves/last.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final submission is now located in data/test1.de-en.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-13T19:42:18.745Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 8\n",
      "\tTrain loss: 2.992\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.4 seconds\n",
      "\tETA = 10054.1 seconds\n",
      "\tMetrics:{'BLEU': 26.432411396040642}\n",
      "100%|#######################################| 1385/1385 [03:46<00:00,  6.12it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.73it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[560]: and i put away the newspaper -- and i was getting on a plane -- and i sat there , and i did something i hadn 't done for a long time -- which is i did nothing .\n",
      "translations[560]: and i -- i put the newspaper aside -- i just got into an airplane -- and then i sat there and i did something i didn 't do for a long time -- which is nothing .\n",
      "original_sentences[84]: well , the supreme court considered this 100-years tradition and said , in an opinion written by justice douglas , that the causbys must lose .\n",
      "translations[84]: the this tradition , and decided to write a of judge that the have lost .\n",
      "original_sentences[12894]: and so it was then that i realized there was an opportunity for me to collaborate with these scientists , and so i jumped at that opportunity .\n",
      "translations[12894]: and then i found that this was an opportunity to work that i liked to do , and as a of\n",
      "original_sentences[10428]: this guy is now five generations removed from the general ; this guy is about seven .\n",
      "translations[10428]: this guy is now five generations from general , this guy is about seven . of the of\n",
      "original_sentences[8992]: you couldn 't really make any money selling marijuana .\n",
      "translations[8992]: the didn 't have much more than that ,\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 9\n",
      "\tTrain loss: 2.992\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 240.3 seconds\n",
      "\tETA = 9853.2 seconds\n",
      "\tMetrics:{'BLEU': 26.635949310280317}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.13it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.75it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[3377]: it 's a cognitive illusion that we 've been studying in my lab for the past few years , and 80 percent of us have it .\n",
      "translations[3377]: this is a cognitive deception that we 've studied in my lab for the last few years , and 80 percent of us in ways , and then , as a of\n",
      "original_sentences[2427]: and the most magnificent part of all that is to go outside on a spacewalk .\n",
      "translations[2427]: and the greatest of all , of all , is a ,\n",
      "original_sentences[3544]: -- that we 've taken this pop icon and we have just tweaked her a little bit to become an ambassador who can carry the message that being a woman scientist studying treetops is actually a really great thing .\n",
      "translations[3544]: that we took this and just a little bit of her , so that she can be an ambassador for it , that it 's actually a great thing to be a scientist studying the canopy , of\n",
      "original_sentences[2531]: let 's start at the beginning .\n",
      "translations[2531]: let 's start from the front . of the\n",
      "original_sentences[11802]: not quite the whole truth . anyway .\n",
      "translations[11802]: not the full truth . as well , as\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 10\n",
      "\tTrain loss: 2.991\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.8 seconds\n",
      "\tETA = 9590.5 seconds\n",
      "\tMetrics:{'BLEU': 26.762400771668307}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.15it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.79it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[6046]: and there are these guys who used to model the future of interest rates , and all that kind of stuff .\n",
      "translations[6046]: there are these guys who used to create models for the future of and things like this .\n",
      "original_sentences[10925]: i 'm not going to make a claim about who 's right , but i will make an empirical claim about people 's intuitions , which is that , if you like the work of jackson pollock , you 'll tend more so than the people who don 't like it to believe that these works are difficult to create , that they require a lot of time and energy and creative energy .\n",
      "translations[10925]: i 'm not going to speak of standards who are right , but i 'm going to make an assumption about their , which means if you like jackson 's work , they tend to tend to believe that these works are difficult to create that they need a lot of time and energy , and creative energy , ,\n",
      "original_sentences[8451]: each one would describe a different universe with different laws of physics .\n",
      "translations[8451]: each version describes another universe with other physical laws , with ,\n",
      "original_sentences[10787]: it 's physically possible to exercise more .\n",
      "translations[10787]: it 's physically possible to move more . this is a of\n",
      "original_sentences[14655]: i 've done most of this work at johns hopkins university , but also at the national institute of health where i was previously .\n",
      "translations[14655]: most of this work i did at john hopkins university , but also at the national institute of health , where i was before . and as a result , i\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 11\n",
      "\tTrain loss: 2.989\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.1 seconds\n",
      "\tETA = 9324.9 seconds\n",
      "\tMetrics:{'BLEU': 26.96138413136253}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.14it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.82it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[15368]: watch her horses . .\n",
      "translations[15368]: to their horses .\n",
      "original_sentences[2948]: thank you very much .\n",
      "translations[2948]: thank you .\n",
      "original_sentences[18897]: seen from another point of view , it gives quite a different impression .\n",
      "translations[18897]: so from another perspective , he 's going to be something very different .\n",
      "original_sentences[16797]: the state 's not alone . there are many , many actors .\n",
      "translations[16797]: the state is not alone . there are many other actors , and the of the of\n",
      "original_sentences[16144]: but when my father came , he would sell the cows , he would sell the products we had , and he went and drank with his friends in the bars .\n",
      "translations[16144]: but when my father came , he sold the cows , he sold our , and he went out and he was drinking with his friends in the , and then ,\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 12\n",
      "\tTrain loss: 2.989\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.3 seconds\n",
      "\tETA = 9092.5 seconds\n",
      "\tMetrics:{'BLEU': 26.782459033312243}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.13it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.84it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[8919]: here is a three-week papercutting marathon at the museum of arts and design in new york city .\n",
      "translations[8919]: this is a at the museum of art and design in new york city .\n",
      "original_sentences[18743]: now biodegradability is a material property ;\n",
      "translations[18743]: biological is a .\n",
      "original_sentences[15997]: it 's absolutely phenomenal . the only downside is that it requires you to know how to text -- send a text message . nobody over 40 knows how to do that .\n",
      "translations[15997]: this is really phenomenal . the only is that you have to know how to write a text and send it out , and nobody over 40 knows how this goes .\n",
      "original_sentences[4706]: and then our mothers tell us , \" careful you don ’ t fall down \" -- because you ’ ll fall over .\n",
      "translations[4706]: and our mothers tell us that if you don 't get , because you 're by ,\n",
      "original_sentences[13525]: i thank you very much .\n",
      "translations[13525]: thank you very much .\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 13\n",
      "\tTrain loss: 2.988\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.9 seconds\n",
      "\tETA = 8874.7 seconds\n",
      "\tMetrics:{'BLEU': 27.326408678179}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.14it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.64it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[16720]: let 's get back to that . and the idea , too , of jews , christians and muslims -- these traditions now so often at loggerheads -- working together to create a document which we hope will be signed by a thousand , at least , of major religious leaders from all the traditions of the world .\n",
      "translations[16720]: let 's go back to this . and also to the idea that jews , christians and muslims , these very often -- working together to create a document that would hopefully be at least a thousand important of all the world 's world 's ,\n",
      "original_sentences[14824]: 5,000 days without interruption -- that 's just unbelievable .\n",
      "translations[14824]: 5,000 days without -- this is incredible .\n",
      "original_sentences[17713]: people will send you donations when you 're in that condition .\n",
      "translations[17713]: people will send them to their hands when they 're able to do this .\n",
      "original_sentences[10417]: you don 't plan the details , and people will figure out what to do , how to adapt to this new framework .\n",
      "translations[10417]: they don 't plan the details , and people will figure out what to do , how to adapt to these new , and then , as a\n",
      "original_sentences[13215]: and toward the end of writing that book there was a documentary that came out .\n",
      "translations[13215]: and when i finished the book , a came out .\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 14\n",
      "\tTrain loss: 2.987\n",
      "\tVal loss: 3.112\n",
      "\tEpoch time = 239.3 seconds\n",
      "\tETA = 8616.2 seconds\n",
      "\tMetrics:{'BLEU': 27.238333432331242}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.15it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.50it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[1497]: and with that , we have found ourselves to be the world 's policemen .\n",
      "translations[1497]: and at the same time , we find ourselves in the role of the ,\n",
      "original_sentences[13074]: you should be a one-buttock player . \"\n",
      "translations[13074]: you should be a . \" as a of\n",
      "original_sentences[2879]: if the connection is weak , the motors will stay off and the fly will continue straight on its course .\n",
      "translations[2879]: if the connection is weak , the motors will continue , and the fly will continue their course , as a of\n",
      "original_sentences[14258]: we thrive not when we 've done it all , but when we still have more to do .\n",
      "translations[14258]: we have no success when we did everything , but if we have more to do more . so ,\n",
      "original_sentences[17767]: i 'll show you how to clap to this song .\n",
      "translations[17767]: i 'm going to show you how to this song .\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 15\n",
      "\tTrain loss: 2.986\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 238.7 seconds\n",
      "\tETA = 8353.5 seconds\n",
      "\tMetrics:{'BLEU': 26.310751974434837}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.15it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.41it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[7316]: commerce is ancient . markets are very old . capitalism is fairly recent ; socialism emerged as a reaction to that .\n",
      "translations[7316]: trade has been there since . markets are very old . capitalism is quite young . budrus came to this .\n",
      "original_sentences[1380]: hopefully this technology will let us have more of these survivors in the future .\n",
      "translations[1380]: hopefully , this technology will allow us to see more of these survivors in the future , as a\n",
      "original_sentences[853]: so three questions i 'd offer you .\n",
      "translations[853]: i have three questions on them . i have a of\n",
      "original_sentences[10173]: thank you for your attention .\n",
      "translations[10173]: thank you very much for your attention .\n",
      "original_sentences[3070]: leading the way -- what 's leading the way ?\n",
      "translations[3070]: and most of us eat -- you guess what , if you can do this :\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 16\n",
      "\tTrain loss: 2.986\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 239.1 seconds\n",
      "\tETA = 8127.9 seconds\n",
      "\tMetrics:{'BLEU': 26.88897197366753}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.54it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[14552]: why is it hard to treat ?\n",
      "translations[14552]: why is cancer so hard to treat cancer ?\n",
      "original_sentences[12004]: we 're all in this together .\n",
      "translations[12004]: we 're all in the same . we 're\n",
      "original_sentences[9848]: tamil is a south indian language , and i said , can tamil-speaking children in a south indian village learn the biotechnology of dna replication in english from a streetside computer ?\n",
      "translations[9848]: is a language , and i said , \" can children of a village learn the biotechnology in english from a computer on the side of the road , \" can\n",
      "original_sentences[13214]: so first , the first thing i want to tell you is that there is a brain region in the human brain , in your brains , whose job it is to think about other people 's thoughts .\n",
      "translations[13214]: first of all , i want to tell you that there 's a in the human brain -- in their brains -- whose task is thinking about the idea of other people 's thoughts\n",
      "original_sentences[2969]: they have their own private sort of army around them at the oil fields .\n",
      "translations[2969]: they have something like their own army around the outside of the .\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 17\n",
      "\tTrain loss: 2.984\n",
      "\tVal loss: 3.112\n",
      "\tEpoch time = 238.3 seconds\n",
      "\tETA = 7862.4 seconds\n",
      "\tMetrics:{'BLEU': 26.620877014848443}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.47it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[9024]: and at the end of the day , these things make for a lifetime of challenge and reward .\n",
      "translations[9024]: and finally , it makes a life with challenges and rewards .\n",
      "original_sentences[909]: it was developed by a company called within technologies and 3t rpd .\n",
      "translations[909]: it was developed by a company called technologies and , , of\n",
      "original_sentences[10779]: and when it dies , it falls to the bottom and then it rots , which means that bacteria break it down .\n",
      "translations[10779]: and when it dies , it falls to the ground and there , which means that bacteria it . of\n",
      "original_sentences[15623]: do you notice what 's bubbling up for you and trying to emerge ?\n",
      "translations[15623]: do you notice what you for opportunities ?\n",
      "original_sentences[3034]: the questions i would like to talk about are : one , where did we come from ?\n",
      "translations[3034]: the questions that i 'd like to talk about is : where are we coming from ?\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 18\n",
      "\tTrain loss: 2.984\n",
      "\tVal loss: 3.112\n",
      "\tEpoch time = 238.4 seconds\n",
      "\tETA = 7629.1 seconds\n",
      "\tMetrics:{'BLEU': 26.519312844083004}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.56it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.48it/s]\n",
      "original_sentences[13557]: so all of a sudden , this labor of love for no obvious economic reason has become a critical piece of information in terms of how that system is going to recover , how long it will take and how the lawsuits and the multi-billion-dollar discussions that are going to happen in the coming years are likely to be resolved .\n",
      "translations[13557]: so suddenly , this was a really obvious economic reason for not a significant piece of information , in terms of how this system , how long it takes and how the court and the could go out in the coming years .\n",
      "original_sentences[19573]: in contrast , when you hear somebody laughing in a posed way , what you see are these regions in pink , which are occupying brain areas associated with mentalizing , thinking about what somebody else is thinking .\n",
      "translations[19573]: if you hear about it , you see this in pink : areas of the brain , where you think about what other thinking , and then you see , as you\n",
      "original_sentences[6371]: now , this is starting to happen in transistors .\n",
      "translations[6371]: the same thing is happening in transistors .\n",
      "original_sentences[4101]: we know how to fix hunger .\n",
      "translations[4101]: as you know how to satisfy hunger .\n",
      "original_sentences[4969]: but the main concern for the environmentally-interested students -- and they are right -- is about the future .\n",
      "translations[4969]: but the students ' -- and they 're right -- is the future . the of the\n",
      "New best model, saving...\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.60it/s]\n",
      "Epoch: 19\n",
      "\tTrain loss: 2.983\n",
      "\tVal loss: 3.111\n",
      "\tEpoch time = 238.4 seconds\n",
      "\tETA = 7390.4 seconds\n",
      "\tMetrics:{'BLEU': 27.153609792365614}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.16it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:12<00:00, 11.90it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:43<00:00,  1.48it/s]\n",
      "original_sentences[16408]: you show me your search history , and i 'll find something incriminating or something embarrassing there in five minutes .\n",
      "translations[16408]: show me your , and i find a little bit of a or a , of the\n",
      "original_sentences[1586]: we started playing with this , and that afternoon , we realized that we could use this new , universal interface to allow anyone in the world to operate the robot in our lab .\n",
      "translations[1586]: we started to use this afternoon , and we realized that we could use this new , universal interface to make it possible for everyone in the world to control the robot in our lab .\n",
      "original_sentences[15278]: edi is real .\n",
      "translations[15278]: edi is a real robot .\n",
      "original_sentences[5150]: when i first heard from ted and chris a few months ago about the possibility of the prize , i was completely floored .\n",
      "translations[5150]: when i first heard the first time from ted and chris a few months ago about the price of the price , i was completely surprised .\n",
      "original_sentences[14883]: keep up with derek .\n",
      "translations[14883]: so , do with derek .\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.60it/s]\n",
      "Epoch: 20\n",
      "\tTrain loss: 2.982\n",
      "\tVal loss: 3.115\n",
      "\tEpoch time = 238.4 seconds\n",
      "\tETA = 7151.5 seconds\n",
      "\tMetrics:{'BLEU': 27.171305273163192}\n",
      "100%|#######################################| 1385/1385 [03:44<00:00,  6.17it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:12<00:00, 11.92it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[17466]: audience : \" can you read this ? \"\n",
      "translations[17466]: audience : \" can you read this ? \" as a of\n",
      "original_sentences[16687]: i wanted to -- well , one of the biggest changes is , 200 years ago , we began using coal from underground , which has a lot of pollution , and 100 years ago , began getting gasoline from underground , with a lot of pollution . and gasoline consumption , or production , will reach its limit in about ten years , and then go down , and we wonder what 's going to happen with transportation .\n",
      "translations[16687]: i wanted -- yes , one of the biggest changes in the world was 200 years ago when we started to promote coal , which led to pollution , and 100 years ago , we took gas from the ground , which is what much more , and consumption , is going to reach off the production of gas , and then go down to about 10 years ago , and you wonder what happens to the\n",
      "original_sentences[3782]: and that was the pile , by the way .\n",
      "translations[3782]: this is , by the way , the stack of\n",
      "original_sentences[10790]: i tried to think , \" how do i respond to that hopelessness ?\n",
      "translations[10790]: how should i answer these ?\n",
      "original_sentences[6237]: and i say , that is your problem .\n",
      "translations[6237]: and i say that 's your problem .\n",
      "100%|███████████████████████████████████████████| 24/24 [00:15<00:00,  1.60it/s]\n",
      "Epoch: 21\n",
      "\tTrain loss: 2.981\n",
      "\tVal loss: 3.113\n",
      "\tEpoch time = 238.3 seconds\n",
      "\tETA = 6910.1 seconds\n",
      "\tMetrics:{'BLEU': 27.08328128027257}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.15it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.81it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [01:44<00:00,  1.47it/s]\n",
      "original_sentences[17233]: it 's not the material goods we want . it 's the rewards we want .\n",
      "translations[17233]: we don 't want the material goods . we want rewards . we want to\n",
      "original_sentences[5808]: bf : yeah .\n",
      "translations[5808]: bf : yeah . of\n",
      "original_sentences[3324]: but there was nothing for this cause , so we married growing a mustache with prostate cancer , and then we created our tagline , which is , \" changing the face of men 's health . \"\n",
      "translations[3324]: but there was nothing for this purpose , so we married with prostate cancer , and then we created our slogan , which is , \" change the face of the man 's health . \" and \" ,\n",
      "original_sentences[7993]: i want to make this as a local sanitary pad movement across the globe . that 's why i put all the details on public domain like an open software .\n",
      "translations[7993]: i want it to be a global , and so all the details are free to use like a open software .\n",
      "original_sentences[1699]: so here it is , a crowd-sourced , six-word summary of a thousand tedtalks at the value of $ 99.50 : \" why the worry ? i 'd rather wonder . \"\n",
      "translations[1699]: so here she is , a collective summary of 1,000 tedtalks into six words worth of dollars : \" why ? i rather ask myself . \"\n",
      "100%|███████████████████████████████████████████| 24/24 [00:14<00:00,  1.61it/s]\n",
      "Epoch: 22\n",
      "\tTrain loss: 2.980\n",
      "\tVal loss: 3.112\n",
      "\tEpoch time = 239.1 seconds\n",
      "\tETA = 6693.4 seconds\n",
      "\tMetrics:{'BLEU': 27.36932137591268}\n",
      "100%|#######################################| 1385/1385 [03:45<00:00,  6.14it/s]\n",
      "100%|█████████████████████████████████████████| 154/154 [00:13<00:00, 11.46it/s]\n",
      " 33%|█████████████▉                            | 51/154 [00:36<01:08,  1.49it/s]^C\n",
      " 33%|█████████████▉                            | 51/154 [00:36<01:14,  1.39it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/main.py\", line 510, in <module>\n",
      "    main(config, args.load)\n",
      "  File \"/kaggle/working/main.py\", line 450, in main\n",
      "    metrics    = calc_metrics(model, valid_dl, tgt_vocab, special_symbols, valid_iterator, config.gen.train, config.gen)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/kaggle/working/main.py\", line 234, in calc_metrics\n",
      "    translations = generate_translations(model, val_dl, special_symbols, tgt_vocab, gen_method, gen_config)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/kaggle/working/main.py\", line 206, in generate_translations\n",
      "    tgt_batch = greedy_decode_multi(model, src_batch, src_mask, special_symbols, max_length)\n",
      "  File \"/kaggle/working/main.py\", line 112, in greedy_decode_multi\n",
      "    output = model.decode(tgt_batch, memory, tgt_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/kaggle/working/src/model.py\", line 158, in decode\n",
      "    return self.model.decode(tgt, memory, tgt_mask)\n",
      "  File \"/kaggle/working/src/model.py\", line 116, in decode\n",
      "    return self.transformer.decoder(pos_enc, memory, tgt_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\", line 494, in forward\n",
      "    output = mod(output, memory, tgt_mask=tgt_mask,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\", line 892, in forward\n",
      "    x = self.norm3(x + self._ff_block(x))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1528, in _wrapped_call_impl\n",
      "    def _wrapped_call_impl(self, *args, **kwargs):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# !python3 main.py --exp_name ed4_mf7_en02_fine --load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# files = ['main.py', 'src/model.py', 'src/data.py', '2025-03-03/best.pt', 'requirements.txt', '']\n",
    "# arc = 'submission.zip'\n",
    "\n",
    "# shutil.make_archive(archive_name.replace('.zip', ''), 'zip', root_dir='.', base_dir=None, files=files_to_archive)\n",
    "\n",
    "# print(f\"Archive created: {archive_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6781393,
     "sourceId": 10909509,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
